import torch as torch

# I've removed the comments here, read the article's annotated versions of each
# piece of code if there's something that's not clear!

def model(rectangle, hidden_layer):
    output_neuron = 0.
    for index, input_neuron in enumerate(rectangle):
        output_neuron += input_neuron * hidden_layer[index]
    return output_neuron

def train(rectangles, hidden_layer):
  outputs = []
  for rectangle in rectangles:
      output = model(rectangle, hidden_layer)
      outputs.append(output)

  error = mean_squared_error(outputs, rectangle_average)

  error.backward()

  for index, _ in enumerate(hidden_layer):
    learning_rate = 0.1
    hidden_layer.data[index] -= learning_rate * hidden_layer.grad.data[index]

  hidden_layer.grad.zero_()
  return error

# We use tensors now, but we just use them as if they were normal lists.
# We only use them so we can get the gradients.
hidden_layer = torch.tensor([0.98, 0.4, 0.86, -0.08], requires_grad=True)

for epoch in range(100):
   error = train(rectangles, hidden_layer)
   # Remember, this is where you expect the layer to trend towards:
   # [0.25, 0.25, 0.25, 0.25] to implement average. If it's not quite there
   # yet, it's because the randomly generated data you got is different.
   # Run it for more epochs. If it doesn't trend correctly, adjust the learning
   # rate (it's stepping past the valley!)
   print(f"Epoch: {epoch}, Error: {error}, Layer: {hidden_layer.data}\n\n")

print(f"After: {model([0.2,0.5,0.4,0.7], hidden_layer)}")
